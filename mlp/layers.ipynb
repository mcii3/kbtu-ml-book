{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers of MLP\n",
    "\n",
    "From mathematical point of view MLP is a smooth function $F$ which is constructed as a composition of some other functions\n",
    "\n",
    "$$\n",
    "F(\\boldsymbol x) = (f_{L} \\circ f_{L-1} \\circ\\ldots \\circ f_2 \\circ f_1)(\\boldsymbol x),\\quad\n",
    "\\boldsymbol x \\in \\mathbb R^{n_0}\n",
    "$$\n",
    "\n",
    "Each function \n",
    "\n",
    "$$\n",
    "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\to \\mathbb R^{n_\\ell}\n",
    "$$\n",
    "\n",
    "is called a **layer**; it converts representation of $(\\ell-1)$-th layer \n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{\\ell -1} \\in \\mathbb R^{n_{\\ell - 1}} \n",
    "$$\n",
    "\n",
    "to the representation of $\\ell$-th layer \n",
    "\n",
    "$$\n",
    "   \\boldsymbol x_{\\ell} \\in \\mathbb R^{n_{\\ell}}.\n",
    "$$\n",
    "\n",
    "Thus, the **input layer** $\\boldsymbol x_0 \\in \\mathbb R^{n_0}$ is converted to the **output layer** $\\boldsymbol x_L \\in \\mathbb R^{n_L}$. All other layers $\\boldsymbol x_\\ell$, $1\\leqslant \\ell < L$, are called **hidden layers**.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/354817375/figure/fig2/AS:1071622807097344@1632506195651/Multi-layer-perceptron-MLP-NN-basic-Architecture.jpg\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "The terminology about layers is a bit ambiguous. Both functions $f_\\ell$ and their outputs $\\boldsymbol x_\\ell = f(\\boldsymbol x_{\\ell - 1})$ are called $\\ell$-th layer in different sources.\n",
    "```\n",
    "\n",
    "## Parameters of MLP\n",
    "\n",
    "However, one important element is missing in this description of MLP: parameters! Each layer $f_\\ell$ has a vector of parameters $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$ (sometimes empty). Hence, a layer should be defined as\n",
    "\n",
    "$$\n",
    "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\times \\mathbb R^{m_\\ell} \\to \\mathbb R^{n_\\ell}.\n",
    "$$\n",
    "\n",
    "The representation $\\boldsymbol x_\\ell$ is calculated from $\\boldsymbol x_{\\ell -1}$ by the formula \n",
    "\n",
    "$$\n",
    "\\boldsymbol x_\\ell = f_\\ell(\\boldsymbol x_{\\ell - 1},\\boldsymbol \\theta_\\ell)\n",
    "$$\n",
    "\n",
    "with some fixed $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$. The whole MLP $F$ depends on parameters of all layers:\n",
    "\n",
    "$$\n",
    "    F(\\boldsymbol x, \\boldsymbol \\theta), \\quad \\boldsymbol \\theta = (\\boldsymbol \\theta_1, \\ldots, \\boldsymbol \\theta_L).\n",
    "$$\n",
    "\n",
    "All these parameters are trained simultaneously by the {ref}`backpropagation method <backprop>`.\n",
    "\n",
    "\n",
    "## Dense layer\n",
    "\n",
    "Edges between two consequetive layers denote **linear** (or **dense**) layer:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_\\ell = f(\\boldsymbol x_{\\ell - 1}; \\boldsymbol W, \\boldsymbol b) = \\boldsymbol {Wx}_{\\ell - 1} + \\boldsymbol b.\n",
    "$$\n",
    "\n",
    "The matrix $\\boldsymbol W \\in \\mathbb R^{n_{\\ell - 1}\\times n_\\ell}$ and vector $\\boldsymbol b \\in \\mathbb R^{n_\\ell}$ (bias) are parameters of the linear layer which defines the linear transformation from $\\boldsymbol x_{\\ell - 1}$ to $\\boldsymbol x_{\\ell}$.\n",
    "\n",
    "**Q**. How many numeric parameters does such linear layer have?\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "\n",
    "Suppose that we apply one more dense layer:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{\\ell + 1} = \\boldsymbol {W'x}_{\\ell} + \\boldsymbol{b'}\n",
    "$$\n",
    "\n",
    "Express $\\boldsymbol x_{\\ell + 1}$ as a function of $\\boldsymbol x_{\\ell - 1}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0762,  0.6222, -1.1178,  0.9935,  2.0089])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4328,  0.1590, -0.0809,  0.3756, -0.3382,  0.4353],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 6)\n",
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
