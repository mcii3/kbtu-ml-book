{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d159bf",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "\n",
    "Classification metrics are used to evaluate the performance of classification models, which are machine learning models that predict categorical labels or classes for input data. \n",
    "<br>\n",
    "\n",
    "These metrics help measure how well a classification model can discriminate between different classes and make correct predictions."
    "Classification metrics are used to evaluate the performance of classification models, which are machine learning models that predict categorical labels or classes for input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb325d",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "The most common metric for binary and multiclass classification which shows the fraction of correct predictions:\n",
    "\n",
    "$$\n",
    "    \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "More formally, if $(\\boldsymbol x_i, y_i)_{i=1}^n$ is the train (or test) dataset, then the accuracy metric is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\mathrm{acc}(\\boldsymbol y, \\boldsymbol {\\widehat y}) = \\frac 1n \\sum\\limits_{i=1}^n \\mathbb I[y_i = \\widehat y_i].\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da978a94",
   "metadata": {},
   "source": [
    "##### Here are some common classification metrics:\n",
    "\n",
    "###### 1. Accuracy Score\n",

    "Here are some common classification metrics:\n",
    "\n",
    "Accuracy Score\n",
    "\n",
    "\n",
    "   Certainly, here is the accuracy score formula expressed in mathematical notation: \n",
    "    \n",
    "\n",
    "$$\n",
    "    \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",

    "\n",

    "    \n",
    "   In mathematical terms:\n",
    "    \n",
    "   \n",
    "$$\n",
    "    \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "    \n",
    "    \n",
    "   Where:\n",
    "\n",
    "\n",
    "* TP (True Positives) represents the number of correctly predicted positive instances.\n",
    "* TN (True Negatives) represents the number of correctly predicted negative instances.\n",
    "* FP (False Positives) represents the number of instances that were actually negative but were predicted as positive.\n",
    "* FN (False Negatives) represents the number of instances that were actually positive but were predicted as negative.\n",
    "\n",
    "\n",
    "######  2. Precision\n",
    "\n",
    "   Precision, also known as positive predictive value, measures the proportion of true positive predictions among all positive predictions made by the model. It is useful when minimizing false positives is a priority.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "######  3. Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "Recall measures the proportion of true positive predictions among all actual positive instances. It is particularly important when minimizing false negatives is crucial.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "\n",
    "######  4. F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when there is an imbalance between the classes.\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e603dd",
   "metadata": {},
   "source": [
    "###### Now let's use these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51313bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8000\n",
      "Precision: 0.8889\n",
      "Recall: 0.7273\n",
      "F1-Score: 0.8000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# True labels for the test data\n",
    "true_labels = [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n",
    "\n",
    "# Predicted labels by your classifier\n",
    "predicted_labels = [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
    "\n",
    "# Calculate Accuracy Score\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1-Score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d0b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb8906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b7998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599f9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
